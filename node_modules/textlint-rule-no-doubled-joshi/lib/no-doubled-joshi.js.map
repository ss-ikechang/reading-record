{"version":3,"sources":["../src/no-doubled-joshi.js"],"names":["createSurfaceKeyMap","tokens","filter","is助詞Token","reduce","keyMap","token","tokenKey","push","matchExceptionRule","pos_detail_1","surface_form","defaultOptions","min_interval","strict","allow","separatorChars","module","exports","context","options","helper","RuleHelper","minInterval","isStrict","Syntax","report","RuleError","Paragraph","node","isChildNode","Link","Image","BlockQuote","Emphasis","isSentenceNode","type","SentenceSyntax","Sentence","txtParentNode","sentences","children","then","tokenizer","checkSentence","sentence","sentenceSource","StringSource","text","toString","tokenizeForSentence","concatTokens","countableTokens","joshiTokenSurfaceKeyMap","Object","keys","forEach","key","joshiName","indexOf","length","prev","current","startPosition","otherPosition","differenceIndex","originalIndex","originalIndexFromIndex","word_position","index"],"mappings":"AAAA;AACA;;AACA;;AACA;;AACA;;AACA;;AACA;;;;;;AAOA;;;;;;;;AAQA,SAASA,mBAAT,CAA6BC,MAA7B,EAAqC;AACjC;AACA,SAAOA,MAAM,CAACC,MAAP,CAAcC,qBAAd,EAAyBC,MAAzB,CAAgC,UAACC,MAAD,EAASC,KAAT,EAAmB;AACtD;AACA,QAAMC,QAAQ,GAAG,kCAAiBD,KAAjB,CAAjB;;AACA,QAAI,CAACD,MAAM,CAACE,QAAD,CAAX,EAAuB;AACnBF,MAAAA,MAAM,CAACE,QAAD,CAAN,GAAmB,EAAnB;AACH;;AACDF,IAAAA,MAAM,CAACE,QAAD,CAAN,CAAiBC,IAAjB,CAAsBF,KAAtB;AACA,WAAOD,MAAP;AACH,GARM,EAQJ,EARI,CAAP;AASH;;AAED,SAASI,kBAAT,CAA4BR,MAA5B,EAAoC;AAChC,MAAIK,KAAK,GAAGL,MAAM,CAAC,CAAD,CAAlB,CADgC,CAEhC;;AACA,MAAIK,KAAK,CAACI,YAAN,KAAuB,KAA3B,EAAkC;AAC9B,WAAO,IAAP;AACH,GAL+B,CAMhC;;;AACA,MAAIJ,KAAK,CAACI,YAAN,KAAuB,KAAvB,IAAgCJ,KAAK,CAACK,YAAN,KAAuB,GAA3D,EAAgE;AAC5D,WAAO,IAAP;AACH,GAT+B,CAUhC;;;AACA,MAAIL,KAAK,CAACI,YAAN,KAAuB,MAAvB,IAAiCJ,KAAK,CAACK,YAAN,KAAuB,GAA5D,EAAiE;AAC7D,WAAO,IAAP;AACH;;AACD,SAAO,KAAP;AACH;AAED;;;;;AAGA,IAAMC,cAAc,GAAG;AACnBC,EAAAA,YAAY,EAAE,CADK;AAEnBC,EAAAA,MAAM,EAAE,KAFW;AAGnBC,EAAAA,KAAK,EAAE,EAHY;AAInBC,EAAAA,cAAc,EAAE,CAAC,GAAD,EAAM,GAAN,EAAW,GAAX,EAAgB,GAAhB,EAAqB,GAArB;AAJG,CAAvB;AAOA;;;;;;;;;AAQAC,MAAM,CAACC,OAAP,GAAiB,UAAUC,OAAV,EAAiC;AAAA,MAAdC,OAAc,uEAAJ,EAAI;AAC9C,MAAMC,MAAM,GAAG,IAAIC,8BAAJ,CAAeH,OAAf,CAAf,CAD8C,CAE9C;;AACA,MAAMI,WAAW,GAAGH,OAAO,CAACP,YAAR,IAAwBD,cAAc,CAACC,YAA3D;AACA,MAAMW,QAAQ,GAAGJ,OAAO,CAACN,MAAR,IAAkBF,cAAc,CAACE,MAAlD;AACA,MAAMC,KAAK,GAAGK,OAAO,CAACL,KAAR,IAAiBH,cAAc,CAACG,KAA9C;AACA,MAAMC,cAAc,GAAGI,OAAO,CAACJ,cAAR,IAA0BJ,cAAc,CAACI,cAAhE;AAN8C,MAOvCS,MAPuC,GAOVN,OAPU,CAOvCM,MAPuC;AAAA,MAO/BC,MAP+B,GAOVP,OAPU,CAO/BO,MAP+B;AAAA,MAOvBC,SAPuB,GAOVR,OAPU,CAOvBQ,SAPuB;AAQ9C,6BACKF,MAAM,CAACG,SADZ,YACuBC,IADvB,EAC6B;AACrB,QAAIR,MAAM,CAACS,WAAP,CAAmBD,IAAnB,EAAyB,CAACJ,MAAM,CAACM,IAAR,EAAcN,MAAM,CAACO,KAArB,EAA4BP,MAAM,CAACQ,UAAnC,EAA+CR,MAAM,CAACS,QAAtD,CAAzB,CAAJ,EAA+F;AAC3F;AACH;;AACD,QAAMC,cAAc,GAAG,SAAjBA,cAAiB,CAAAN,IAAI,EAAI;AAC3B,aAAOA,IAAI,CAACO,IAAL,KAAcC,yBAAeC,QAApC;AACH,KAFD;;AAGA,QAAMC,aAAa,GAAG,gCAAeV,IAAf,CAAtB;AACA,QAAMW,SAAS,GAAGD,aAAa,CAACE,QAAd,CAAuBvC,MAAvB,CAA8BiC,cAA9B,CAAlB;AACA,WAAO,+BAAeO,IAAf,CAAoB,UAAAC,SAAS,EAAI;AACpC,UAAMC,aAAa,GAAG,SAAhBA,aAAgB,CAACC,QAAD,EAAc;AAChC,YAAMC,cAAc,GAAG,IAAIC,gCAAJ,CAAiBF,QAAjB,CAAvB;AACA,YAAMG,IAAI,GAAGF,cAAc,CAACG,QAAf,EAAb;AACA,YAAMhD,MAAM,GAAG0C,SAAS,CAACO,mBAAV,CAA8BF,IAA9B,CAAf,CAHgC,CAIhC;AACA;AACA;AACA;;AACA,YAAMG,YAAY,GAAG,oCAAmBlD,MAAnB,CAArB;AACA,YAAMmD,eAAe,GAAGD,YAAY,CAACjD,MAAb,CAAoB,UAAAI,KAAK,EAAI;AACjD,cAAIkB,QAAJ,EAAc;AACV,mBAAO,2BAAUlB,KAAV,CAAP;AACH,WAHgD,CAIjD;AACA;AACA;;;AACA,iBAAO,2BAAUA,KAAV,KAAoB,2BAAUA,KAAV,CAA3B;AACH,SARuB,CAAxB;AASA,YAAM+C,uBAAuB,GAAGrD,mBAAmB,CAACoD,eAAD,CAAnD;AACA;;;;;;;;;AASAE,QAAAA,MAAM,CAACC,IAAP,CAAYF,uBAAZ,EAAqCG,OAArC,CAA6C,UAAAC,GAAG,EAAI;AAChD,cAAMxD,MAAM,GAAGoD,uBAAuB,CAACI,GAAD,CAAtC;AACA,cAAMC,SAAS,GAAG,yCAAwBD,GAAxB,CAAlB,CAFgD,CAGhD;;AACA,cAAI1C,KAAK,CAAC4C,OAAN,CAAcD,SAAd,KAA4B,CAAhC,EAAmC;AAC/B;AACH,WAN+C,CAOhD;;;AACA,cAAI,CAAClC,QAAL,EAAe;AACX,gBAAIf,kBAAkB,CAACR,MAAD,CAAtB,EAAgC;AAC5B;AACH;AACJ;;AACD,cAAIA,MAAM,CAAC2D,MAAP,IAAiB,CAArB,EAAwB;AACpB,mBADoB,CACb;AACV,WAf+C,CAgBhD;AACA;;;AACA3D,UAAAA,MAAM,CAACG,MAAP,CAAc,UAACyD,IAAD,EAAOC,OAAP,EAAmB;AAC7B,gBAAMC,aAAa,GAAGX,eAAe,CAACO,OAAhB,CAAwBE,IAAxB,CAAtB;AACA,gBAAMG,aAAa,GAAGZ,eAAe,CAACO,OAAhB,CAAwBG,OAAxB,CAAtB,CAF6B,CAG7B;;AACA,gBAAMG,eAAe,GAAGD,aAAa,GAAGD,aAAxC;;AACA,gBAAIE,eAAe,IAAI1C,WAAvB,EAAoC;AAChC;AACA,kBAAM2C,aAAa,GAAGpB,cAAc,CAACqB,sBAAf,CAAsCL,OAAO,CAACM,aAAR,GAAwB,CAA9D,CAAtB;AACA1C,cAAAA,MAAM,CAACmB,QAAD,EAAW,IAAIlB,SAAJ,8GAAmC+B,SAAnC,gEAA2D;AACxEW,gBAAAA,KAAK,EAAEH;AADiE,eAA3D,CAAX,CAAN;AAGH;;AACD,mBAAOJ,OAAP;AACH,WAbD;AAcH,SAhCD;AAiCH,OA7DD;;AA8DAtB,MAAAA,SAAS,CAACgB,OAAV,CAAkBZ,aAAlB;AACH,KAhEM,CAAP;AAiEH,GA3EL;AA6EH,CArFD","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport {RuleHelper} from \"textlint-rule-helper\";\nimport {getTokenizer} from \"kuromojin\";\nimport {splitAST as splitSentences, Syntax as SentenceSyntax} from \"sentence-splitter\";\nimport StringSource from \"textlint-util-to-string\";\nimport {\n    is助詞Token, is読点Token,\n    concatJoishiTokens,\n    createKeyFromKey,\n    restoreToSurfaceFromKey\n} from \"./token-utils\";\n\n/**\n * Create token map object\n * {\n *  \"は:助詞.係助詞\": [token, token]\n * }\n * @param tokens\n * @returns {*}\n */\nfunction createSurfaceKeyMap(tokens) {\n    // 助詞のみを対象とする\n    return tokens.filter(is助詞Token).reduce((keyMap, token) => {\n        // \"は:助詞.係助詞\" : [token]\n        const tokenKey = createKeyFromKey(token);\n        if (!keyMap[tokenKey]) {\n            keyMap[tokenKey] = [];\n        }\n        keyMap[tokenKey].push(token);\n        return keyMap;\n    }, {});\n}\n\nfunction matchExceptionRule(tokens) {\n    let token = tokens[0];\n    // \"の\" の重なりは例外\n    if (token.pos_detail_1 === \"連体化\") {\n        return true;\n    }\n    // \"を\" の重なりは例外\n    if (token.pos_detail_1 === \"格助詞\" && token.surface_form === \"を\") {\n        return true;\n    }\n    // 接続助詞 \"て\" の重なりは例外\n    if (token.pos_detail_1 === \"接続助詞\" && token.surface_form === \"て\") {\n        return true;\n    }\n    return false;\n}\n\n/*\n default options\n */\nconst defaultOptions = {\n    min_interval: 1,\n    strict: false,\n    allow: [],\n    separatorChars: [\"。\", \"?\", \"!\", \"？\", \"！\"]\n};\n\n/*\n 1. Paragraph Node -> text\n 2. text -> sentences\n 3. tokenize sentence\n 4. report error if found word that match the rule.\n\n TODO: need abstraction\n */\nmodule.exports = function (context, options = {}) {\n    const helper = new RuleHelper(context);\n    // 最低間隔値\n    const minInterval = options.min_interval || defaultOptions.min_interval;\n    const isStrict = options.strict || defaultOptions.strict;\n    const allow = options.allow || defaultOptions.allow;\n    const separatorChars = options.separatorChars || defaultOptions.separatorChars;\n    const {Syntax, report, RuleError} = context;\n    return {\n        [Syntax.Paragraph](node) {\n            if (helper.isChildNode(node, [Syntax.Link, Syntax.Image, Syntax.BlockQuote, Syntax.Emphasis])) {\n                return;\n            }\n            const isSentenceNode = node => {\n                return node.type === SentenceSyntax.Sentence;\n            };\n            const txtParentNode = splitSentences(node);\n            const sentences = txtParentNode.children.filter(isSentenceNode);\n            return getTokenizer().then(tokenizer => {\n                const checkSentence = (sentence) => {\n                    const sentenceSource = new StringSource(sentence);\n                    const text = sentenceSource.toString();\n                    const tokens = tokenizer.tokenizeForSentence(text);\n                    // 助詞 + 助詞は 一つの助詞として扱う\n                    // https://github.com/textlint-ja/textlint-rule-no-doubled-joshi/issues/15\n                    // 連語(助詞)の対応\n                    // http://www.weblio.jp/parts-of-speech/%E9%80%A3%E8%AA%9E(%E5%8A%A9%E8%A9%9E)_1\n                    const concatTokens = concatJoishiTokens(tokens);\n                    const countableTokens = concatTokens.filter(token => {\n                        if (isStrict) {\n                            return is助詞Token(token);\n                        }\n                        // デフォルトでは、\"、\"を間隔値の距離としてカウントする\n                        // \"、\" があると助詞同士の距離が開くようにすることで、並列的な\"、\"の使い方を許容する目的\n                        // https://github.com/azu/textlint-rule-no-doubled-joshi/issues/2\n                        return is助詞Token(token) || is読点Token(token);\n                    });\n                    const joshiTokenSurfaceKeyMap = createSurfaceKeyMap(countableTokens);\n                    /*\n                     # Data Structure\n\n                     joshiTokens = [tokenA, tokenB, tokenC, tokenD, tokenE, tokenF]\n                     joshiTokenSurfaceKeyMap = {\n                         \"は:助詞.係助詞\": [tokenA, tokenC, tokenE],\n                         \"で:助詞.係助詞\": [tokenB, tokenD, tokenF]\n                     }\n                     */\n                    Object.keys(joshiTokenSurfaceKeyMap).forEach(key => {\n                        const tokens = joshiTokenSurfaceKeyMap[key];\n                        const joshiName = restoreToSurfaceFromKey(key);\n                        // check allow\n                        if (allow.indexOf(joshiName) >= 0) {\n                            return;\n                        }\n                        // strict mode ではない時例外を除去する\n                        if (!isStrict) {\n                            if (matchExceptionRule(tokens)) {\n                                return;\n                            }\n                        }\n                        if (tokens.length <= 1) {\n                            return;// no duplicated token\n                        }\n                        // if found differenceIndex less than\n                        // tokes are sorted ascending order\n                        tokens.reduce((prev, current) => {\n                            const startPosition = countableTokens.indexOf(prev);\n                            const otherPosition = countableTokens.indexOf(current);\n                            // 助詞token同士の距離が設定値以下ならエラーを報告する\n                            const differenceIndex = otherPosition - startPosition;\n                            if (differenceIndex <= minInterval) {\n                                // padding positionを計算する\n                                const originalIndex = sentenceSource.originalIndexFromIndex(current.word_position - 1);\n                                report(sentence, new RuleError(`一文に二回以上利用されている助詞 \"${joshiName}\" がみつかりました。`, {\n                                    index: originalIndex\n                                }));\n                            }\n                            return current;\n                        });\n                    });\n                };\n                sentences.forEach(checkSentence);\n            });\n        }\n    }\n};\n"],"file":"no-doubled-joshi.js"}